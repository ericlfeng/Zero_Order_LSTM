#!/bin/bash

##############################################################################
# Run N-layer LSTM overfitting experiments using BEST LR/Alpha from search
#
# This script reads the best hyperparameters from a CSV file generated by
# parse_lr_search_results.py and runs longer training experiments.
#
# Usage:
#   1. First run the LR search: ./lstm_nlayer_overfit_lr_search.sh
#   2. Parse results: python scripts/parse_lr_search_results.py --wandb_project Zero_Order_Opt_LSTM_LR_Search_2 --output best_lr_alpha.csv
#   3. Run this script: ./scripts/lstm_nlayer_overfit_best_lr.sh
#
##############################################################################

set -e  # Exit on error

########## CONFIGURATION #############
n_layers=3
CSV_FILE="${1:-best_lr_alpha.csv}"  # Default or pass as argument
RUN_PREFIX="lstm${n_layers}L_best"
WANDB_PROJ="Zero_Order_Opt_LSTM_Overfit_Best"

# Training configuration (longer runs)
LOG_INTERVAL=100
MAX_ITERS=5000

# Fixed hyperparameters (same as sweep script)
hidden_size=111
memory_size=111
head_size=0
num_heads=1
input_dim=128
INPUT_SAMPLE_LENGTH=100
MICRO_BS=1
MACRO_BS=1
MAX_NUM=120
WEIGHT_DECAY=0
GRAD_CLIP=0
SANGER_RANK=1
alpha_eye_scalar=1.0
beta_eigen_sanger=0
BETA1=0.
BETA2=0.

# Flags
TIE_EPS_TO_LR=true
ADAM=false
WANDB=true
OVERFIT=true

##############################################################################

# Check if CSV file exists
if [ ! -f "$CSV_FILE" ]; then
    echo "[ERROR] CSV file not found: $CSV_FILE"
    echo ""
    echo "Please generate it first by running:"
    echo "  python scripts/parse_lr_search_results.py --wandb_project Zero_Order_Opt_LSTM_LR_Search_2 --output $CSV_FILE"
    echo ""
    echo "Or specify a different CSV file:"
    echo "  ./scripts/lstm_nlayer_overfit_best_lr.sh /path/to/your_best_lr_alpha.csv"
    exit 1
fi

# 1) Kill existing screen sessions (optional - comment out if you want to keep them)
echo "[INFO] Killing existing screen sessions..."
screen -ls | grep '\.' | awk '{print $1}' | xargs -I{} screen -S {} -X quit 2>/dev/null || true

echo "[INFO] Reading best hyperparameters from: $CSV_FILE"
echo "[INFO] Starting long training runs (${MAX_ITERS} iterations each)..."
echo ""

# Function to truncate a long session name (if needed)
truncate_name() {
  echo "$1" | cut -c1-65
}

# --- Detect Available GPUs ---
echo "[INFO] Detecting available GPUs..."
GPU_IDS=($(nvidia-smi --query-gpu=index --format=csv,noheader 2>/dev/null)) || GPU_IDS=()
NUM_GPUS=${#GPU_IDS[@]}

if [ "$NUM_GPUS" -eq 0 ]; then
    echo "[WARN] No GPUs detected. Using CPU (cuda:0 will fallback)."
    GPU_IDS=(0)
    NUM_GPUS=1
fi
echo "[INFO] Detected ${NUM_GPUS} GPUs with IDs: ${GPU_IDS[@]}"

run_counter=0

# Read CSV and launch experiments
# CSV format: model_scale,solver,num_perturbations,best_learning_rate,best_saturating_alpha,...
echo ""
echo "================================================================"
echo "Configurations to run:"
echo "================================================================"
printf "%-6s | %-10s | %-5s | %-12s | %-10s\n" "Scale" "Solver" "Pert" "LR" "Alpha"
echo "----------------------------------------------------------------"

# First, print the configurations
tail -n +2 "$CSV_FILE" | while IFS=',' read -r model_scale solver num_perturbations best_lr best_alpha rest; do
    printf "%-6s | %-10s | %-5s | %-12s | %-10s\n" "$model_scale" "$solver" "$num_perturbations" "$best_lr" "$best_alpha"
done

echo "================================================================"
echo ""

# Now actually launch the experiments
tail -n +2 "$CSV_FILE" | while IFS=',' read -r model_scale solver num_perturbations best_lr best_alpha best_loss best_acc best_status num_eval num_valid; do
    
    # Skip empty lines or header remnants
    if [ -z "$model_scale" ] || [ "$model_scale" = "model_scale" ]; then
        continue
    fi
    
    # Calculate hidden/memory sizes based on scale
    this_hidden_size=$(( hidden_size * model_scale ))
    this_memory_size=$(( memory_size * model_scale ))
    
    # Set epsilon
    if [ "$TIE_EPS_TO_LR" = true ]; then
        EPS=$best_lr
    else
        EPS=0.1
    fi
    
    # Build run name
    RUN_NAME_BASE="${RUN_PREFIX}_${run_counter}_pert${num_perturbations}_s${model_scale}_${solver}_lr${best_lr}_sa${best_alpha}"
    
    # Build extra flags
    EXTRA_FLAGS=""
    
    if [ "$ADAM" = true ]; then
        EXTRA_FLAGS+=" --use_adam"
    fi
    
    if [ "$OVERFIT" = true ]; then
        EXTRA_FLAGS+=" --overfit_to_one_batch_flag"
    fi
    
    if [ "$WANDB" = true ]; then
        EXTRA_FLAGS+=" --wandb"
        EXTRA_FLAGS+=" --wandb_proj ${WANDB_PROJ}"
        EXTRA_FLAGS+=" --wandb_run_name ${RUN_NAME_BASE}"
    fi
    
    # Assign GPU in round-robin fashion
    gpu_index=$(( run_counter % NUM_GPUS ))
    assigned_gpu_id=${GPU_IDS[$gpu_index]}
    device_string="cuda:${assigned_gpu_id}"
    
    RUN_NAME=$(truncate_name "${RUN_NAME_BASE}")
    echo "[INFO] Launching: $RUN_NAME_BASE (GPU: $assigned_gpu_id)"
    
    screen -dmS "$RUN_NAME" bash -c "
    echo '[INFO] Starting run: $RUN_NAME_BASE';
    export WANDB_RUN_NAME=$RUN_NAME;
    python rge_series_experiments.py \
          --model_type LSTM \
          --device ${device_string} \
          --task copy \
          --seq_length ${INPUT_SAMPLE_LENGTH} \
          --hidden_size ${this_hidden_size} \
          --memory_size ${this_memory_size} \
          --head_size ${head_size} \
          --num_heads ${num_heads} \
          --input_size ${input_dim} \
          --n_layers ${n_layers} \
          --micro_batch_size ${MICRO_BS} \
          --macro_batch_size ${MACRO_BS} \
          --max_iterations ${MAX_ITERS} \
          --log_interval ${LOG_INTERVAL} \
          --learning_rate ${best_lr} \
          --epsilon ${EPS} \
          --sanger_rank ${SANGER_RANK} \
          --weight_decay ${WEIGHT_DECAY} \
          --max_num ${MAX_NUM} \
          --grad_clip ${GRAD_CLIP} \
          --num_perturbations ${num_perturbations} \
          --tokenizer char_level \
          --distribution rad \
          --beta1 ${BETA1} \
          --beta2 ${BETA2} \
          --solver ${solver} \
          --sanger_qr_every 100 \
          --saturating_alpha ${best_alpha} \
          --warmup_iters 1 \
          --seed 42 \
          --alpha_eye_scalar ${alpha_eye_scalar} \
          --beta_eigen_sanger ${beta_eigen_sanger} \
          --output_dir ./results_lstm${n_layers}layer_overfit_best \
          ${EXTRA_FLAGS} \
          ;
    echo '[INFO] Finished run: $RUN_NAME_BASE';
    exec bash
    "
    
    run_counter=$(( run_counter + 1 ))
    
done

# Get the final count (need to re-count since loop runs in subshell)
total_runs=$(tail -n +2 "$CSV_FILE" | wc -l | tr -d ' ')

echo ""
echo "[INFO] Done launching all ${total_runs} experiments."
echo "[INFO] Results will be in:"
echo "       - WandB project: ${WANDB_PROJ}"
echo "       - Local directory: ./results_lstm${n_layers}layer_overfit_best"
echo ""
echo "[INFO] Monitor with: screen -ls"
echo "[INFO] Attach to a session: screen -r <session_name>"

